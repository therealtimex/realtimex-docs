---
title: "What is NVIDIA NIM?"
description: "Learn about how to use NVIDIA NIM on your RTX GPU to speed up local AI inference."
---

import { Callout } from "nextra/components";

<Callout type="warning">
  NVIDIA NIM is currently in **beta** and is only available on Windows 11 on **RealTimeX Desktop v.1.7.8** and above.

  You can download the latest version of RealTimeX Desktop (v.1.7.8+) [here](https://realtimex.ai/download).
</Callout>

# What is NVIDIA NIM?

NVIDIA NIM (aka: Nvidia Inference Microservices) is a software technology, which packages optimized inference engines, industry-standard APIs and support for AI models into containers for easy deployment.

All of this runs via WSL2 on Windows and makes it easy to deploy and run LLM models locally at the fastest speeds possible on RTX AI PC's. RealTimeX features a bespoke integration in the RealTimeX Desktop client that makese
installation, setup, and usage of NIM a breeze.

NVIDIA NIM is currently in **beta** and is only available on Windows 11 on **RealTimeX Desktop**.

## Privacy

NVIDIA NIM models run __fully__ locally on your machine using your own GPU. RealTimeX does not send any data to NVIDIA or any other third party in order to run NIM models.
After a model is installed, it is present on your local machine and RealTimeX will use this local engine for inference.

NVIDIA NIM on RTX is __not to be confused__ with NVIDIA's cloud-based NIM offering. This is a __completely__ separate product and service designed to run NIM on your local RTX GPU.

## How does it work?

A NIM is a single model + software stack, packaged into a container designed and maintained by NVIDIA. It is specificially designed to be run on NVIDIA RTX GPUs.
In RealTimeX, we use NIM to run the LLM models for chat, agents, and all other tasks that require inference.

See the [NVIDIA NIM system requirements](/nvidia-nims/system-requirements) for the full list of requirements to run NIM models on your system.

## What models are supported?

RealTimeX supports all of the models that are available in the NIM containers. You can see the full list of models [on build.nvidia.com](https://build.nvidia.com/search?q=chat-run-on-rtx).

## How do I install it?

RealTimeX will present you with a simple to use UI to install and manage NIM containers if you select the `NVIDIA NIM` LLM provider and are on a compatible operating system.

Once the official NIM installer has finished, you will be able to use NVIDIA NIM models in RealTimeX.

See the [NVIDIA NIM x RealTimeX Walkthrough](/nvidia-nims/walkthrough) for the full walkthrough.

## Definitions

- **NIM**: Nvidia Inference Microservice - a single LLM or Model + software stack, packaged into a container designed and maintained by NVIDIA.
- **WSL2**: Windows Subsystem for Linux 2 - a compatibility layer that allows you to run Linux binaries on Windows 11. You will not need to directly interact with WSL2 - the NIM installer will handle this for you and RealTimeX will use it automatically.
- **NIM Installer**: The pre-built NVIDIA NIM installer that runs in the RealTimeX Desktop client to unlock the use of NIM models in RealTimeX.
- **NIM Manager**: The RealTimeX UI that allows you to install, update, and run a NIM.

---
title: "Import an LLM into RealTimeX"
description: "How to import an LLM into RealTimeX"
---

import { Callout } from "nextra/components";

# Importing custom LLMs into RealTimeX

RealTimeX allows you to easily load any valid `GGUF` file and select that as your LLM with zero-setup. Please use **text-based LLMs only** for this process. Embedder models will not function as chat models.

## Import a model into RealTimeX

<Callout type="warning" emoji="‼️">
  <b>Desktop only!</b>  
  The following steps currently apply to RealTimeX Desktop.
</Callout>

---

## Option 1: Import a Local GGUF Model

1. **Download a `GGUF` model file** (for example, from [Hugging Face](https://huggingface.co/models?library=gguf)).
2. Open RealTimeX Desktop and go to the model management section.
3. Click **"Import Model"** and select your `.gguf` file from your computer.
4. The model will appear in your available LLMs list. Select it to start chatting.

> **Tip:**  
> GGUF models are typically optimized for use with local inference backends such as llama.cpp.

---

## Option 2: Use Ollama to Import and Run Models

Ollama makes it extremely easy to run LLMs locally, and RealTimeX can connect to any Ollama-served model.

**Step-by-step:**

1. **Install Ollama**  
   - Go to [ollama.com/download](https://ollama.com/download) and download/install for your OS.
   - Launch Ollama (just run the app or the `ollama serve` command).

2. **Pull an LLM with Ollama**  
   - Open your terminal and run:  
     ```bash
     ollama pull llama3
     ```
   - Or replace `llama3` with any supported model name (see [Ollama's model library](https://ollama.com/library)).

3. **Connect RealTimeX to Ollama**  
   - In RealTimeX Desktop, go to the LLM setup or model selection screen.
   - Choose **Ollama** as the model provider.
   - RealTimeX will auto-detect running Ollama models. Select the one you want to use.

4. **Start using the model!**  
   - The model will now be available in RealTimeX just like any built-in option.

<Callout type="info" emoji="💡">
  You can run multiple models with Ollama. Just pull each one with `ollama pull modelname` and select the active model in RealTimeX.
</Callout>

---

## Troubleshooting

- If your GGUF file doesn't appear, double-check it’s a **text-generation** model (not embedder).
- If RealTimeX doesn’t detect Ollama, make sure the Ollama server is running.
- You can find more models for Ollama at [ollama.com/library](https://ollama.com/library).


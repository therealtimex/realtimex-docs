---
title: "Developer Guide"
description: "SDK Integration for RealTimeX Local Apps"
---

import { Callout, Tabs } from 'nextra/components'

# Local Apps Developer Guide

This guide covers how to integrate your applications with RealTimeX. Depending on your project requirements, you can choose between two operating modes.


## Prerequisites

Regardless of the mode you choose, you must first configure your Local App in the RealTimeX Main App:

1. Open **RealTimeX** â†’ **Settings** â†’ **Local Apps**
2. Create or configure your Local App
3. Enter Supabase **URL** and **Anon Key**
4. For **Compatible Mode**, click **Login to Supabase** â†’ **Auto-Setup Schema**.

<Callout type="info">
If using Compatible Mode, schema setup is handled entirely by the Main App. **No manual SQL required.**
</Callout>

## Installation

<Tabs items={['TypeScript', 'Python']}>
  <Tabs.Tab>
    ```bash
    npm install @realtimex/sdk
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```bash
    pip install realtimex-sdk
    ```
  </Tabs.Tab>
</Tabs>

## Demo Examples

Check out our complete demo applications showcasing all SDK features:

ðŸ‘‰ **[GitHub: local-app-examples](https://github.com/therealtimex/local-app-examples)**

| Example | Language | Description |
|---------|----------|-------------|
| `nodejs-app` | TypeScript + Express | Full-featured demo with TailwindCSS UI |
| `python-app` | Python + NiceGUI | Interactive demo with real-time UI |

## Quick Start

When you start your Local App from the RealTimeX Main App, environment variables `RTX_APP_ID` and `RTX_APP_NAME` are automatically set. The SDK auto-detects these.

<Tabs items={['TypeScript', 'Python']}>
  <Tabs.Tab>
    ```typescript
    import express from 'express';
    import { RealtimeXSDK } from '@realtimex/sdk';

    const app = express();
    const sdk = new RealtimeXSDK({
        permissions: [
            'api.agents',       // List agents
            'api.workspaces',   // List workspaces
            'api.threads',      // List threads
            'webhook.trigger',  // Trigger agents
            'activities.read',  // Read activities
            'activities.write', // Write activities
            'llm.chat',         // Chat completion
            'llm.embed',        // Generate embeddings
            'vectors.read',     // Query vectors
            'vectors.write',    // Store vectors
        ],
    });
    

    // Get available port (auto-detects or finds free port if conflict)
    const port = await sdk.port.getPort();

    // Insert activity
    const activity = await sdk.activities.insert({
      type: 'new_lead',
      email: 'user@example.com',
    });

    // Trigger agent
    await sdk.webhook.triggerAgent({
      raw_data: activity,
      auto_run: true,
      agent_name: 'processor',
      workspace_slug: 'sales',
      thread_slug: 'general',
    });

    app.listen(port, () => console.log(`Running on port ${port}`));
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```python
    from nicegui import ui
    from realtimex_sdk import RealtimeXSDK

    sdk = RealtimeXSDK(config=SDKConfig(
        permissions=[
            'api.agents',       # List agents
            'api.workspaces',   # List workspaces
            'api.threads',      # List threads
            'webhook.trigger',  # Trigger agents
            'activities.read',  # Read activities
            'activities.write', # Write activities
            'llm.chat',         # Chat completion
            'llm.embed',        # Generate embeddings
            'vectors.read',     # Query vectors
            'vectors.write',    # Store vectors
        ]
    ))

    # Get available port (auto-detects or finds free port if conflict)
    port = sdk.port.get_port()

    # Insert activity
    activity = await sdk.activities.insert({
        "type": "new_lead",
        "email": "user@example.com"
    })

    # Trigger agent
    await sdk.webhook.trigger_agent(
        raw_data=activity,
        auto_run=True,
        agent_name="processor",
        workspace_slug="sales",
        thread_slug="general",
    )

    ui.run(port=port)
    ```
  </Tabs.Tab>
</Tabs>

---

## SDK Features

### Activities CRUD

Manage your `activities` table directly through the SDK without needing direct database access. Any data changes (INSERT, UPDATE, or DELETE) will automatically create a new calendar event. If Automation Agent Handlers are configured, they will also trigger an automated agent task.

<Tabs items={['TypeScript', 'Python']}>
  <Tabs.Tab>
    ```typescript
    // Insert new activity
    const activity = await sdk.activities.insert({
      type: 'order',
      amount: 100
    });

    // List activities with filters
    const pending = await sdk.activities.list({
      status: 'pending',
      limit: 10
    });

    // Get single activity
    const item = await sdk.activities.get('activity-uuid');

    // Update activity
    await sdk.activities.update('activity-uuid', {
      status: 'processed'
    });

    // Delete activity
    await sdk.activities.delete('activity-uuid');
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```python
    # Insert new activity
    activity = await sdk.activities.insert({
        "type": "order",
        "amount": 100
    })

    # List activities with filters
    pending = await sdk.activities.list(
        status="pending",
        limit=10
    )

    # Get single activity
    item = await sdk.activities.get("activity-uuid")

    # Update activity
    await sdk.activities.update("activity-uuid", {
        "status": "processed"
    })

    # Delete activity
    await sdk.activities.delete("activity-uuid")
    ```
  </Tabs.Tab>
</Tabs>

### Webhook & Agent Triggering

Trigger AI Agents to process data either manually or automatically.

<Tabs items={['TypeScript', 'Python']}>
  <Tabs.Tab>
    ```typescript
    // Manual Mode (Default) - Creates calendar event for review
    await sdk.webhook.triggerAgent({
      raw_data: { email: 'user@example.com' },
    });

    // Auto-run Mode - Triggers agent immediately
    await sdk.webhook.triggerAgent({
      raw_data: activity,
      auto_run: true,
      agent_name: 'pdf-processor',
      workspace_slug: 'operations',
      thread_slug: 'general', // or "create_new" to create new thread
      prompt: 'Summarize this file' // Optional
    });
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```python
    # Manual Mode (Default)
    await sdk.webhook.trigger_agent(
        raw_data={"email": "user@example.com"}
    )

    # Auto-run Mode
    await sdk.webhook.trigger_agent(
        raw_data=activity,
        auto_run=True,
        agent_name="pdf-processor",
        workspace_slug="operations",
        thread_slug="general", # or "create_new" to create new thread
        prompt="Summarize this file"
    )
    ```
  </Tabs.Tab>
</Tabs>

### Public Metadata API

Access RealTimeX system metadata like available agents and workspaces.

<Tabs items={['TypeScript', 'Python']}>
  <Tabs.Tab>
    ```typescript
    const agents = await sdk.api.getAgents();
    const workspaces = await sdk.api.getWorkspaces();
    const threads = await sdk.api.getThreads('workspace-slug');
    const taskStatus = await sdk.api.getTask('task-uuid');
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```python
    agents = await sdk.api.get_agents()
    workspaces = await sdk.api.get_workspaces()
    threads = await sdk.api.get_threads("workspace-slug")
    task_status = await sdk.api.get_task("task-uuid")
    ```
  </Tabs.Tab>
</Tabs>

---

### Port Management

The SDK includes automatic port management to prevent conflicts when running multiple Local Apps simultaneously.

<Tabs items={['TypeScript', 'Python']}>
  <Tabs.Tab>
    ```typescript
    // Get an available port (auto-detects from RTX_PORT or finds free port)
    const port = await sdk.port.getPort();
    app.listen(port);
    
    // Or use individual methods
    const suggested = sdk.port.getSuggestedPort(); // RTX_PORT env or 8080
    const available = await sdk.port.isPortAvailable(3000);
    const freePort = await sdk.port.findAvailablePort(8080);
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```python
    # Get an available port (auto-detects from RTX_PORT or finds free port)
    port = sdk.port.get_port()
    ui.run(port=port)  # NiceGUI
    
    # Or use individual methods
    suggested = sdk.port.get_suggested_port()  # RTX_PORT env or 8080
    available = sdk.port.is_port_available(3000)
    free_port = sdk.port.find_available_port(8080)
    ```
  </Tabs.Tab>
</Tabs>

<Callout type="info">
**How it works**: When launched from RealTimeX, your app receives an `RTX_PORT` environment variable with the configured port. The SDK uses this as the starting point and automatically finds the next available port if it's occupied.
</Callout>

---

### LLM Proxy & Vector Store

The SDK provides access to RealtimeX's LLM capabilities without needing to manage API keys directly. Perfect for building RAG (Retrieval-Augmented Generation) applications.

**Required Permissions:**
```typescript
const sdk = new RealtimeXSDK({
  permissions: ['llm.chat', 'llm.embed', 'llm.providers', 'vectors.read', 'vectors.write']
});
```

#### List Available Models

<Tabs items={['TypeScript', 'Python']}>
  <Tabs.Tab>
    ```typescript
    // Get chat (LLM) providers
    const { providers: chatProviders } = await sdk.llm.chatProviders();
    console.log('Chat models:', chatProviders[0].models);

    // Get embedding providers
    const { providers: embedProviders } = await sdk.llm.embedProviders();
    console.log('Embedding models:', embedProviders[0].models);
    ```

  </Tabs.Tab>
  <Tabs.Tab>
    ```python
    # Get chat (LLM) providers
    chatRes = await sdk.llm.chat_providers()
    print(f"Chat models: {chatRes.providers[0].models}")

    # Get embedding providers
    embedRes = await sdk.llm.embed_providers()
    print(f"Embedding models: {embedRes.providers[0].models}")
    ```

  </Tabs.Tab>
</Tabs>

#### Chat Completion

<Tabs items={['TypeScript', 'Python']}>
  <Tabs.Tab>
    ```typescript
    // Sync Chat
    const response = await sdk.llm.chat(
      [
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'user', content: 'What is RealtimeX?' }
      ],
      { 
        model: 'gpt-4o',           // Optional: specific model
        provider: 'openai',        // Optional: specific provider
        temperature: 0.7,          // Optional: 0.0-2.0
        max_tokens: 1000           // Optional: max response tokens
      }
    );
    console.log(response.response?.content);

    // Streaming Chat
    for await (const chunk of sdk.llm.chatStream(messages, options)) {
      process.stdout.write(chunk.textResponse || '');
    }
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```python
    # Sync Chat
    response = await sdk.llm.chat(
        messages=[
            ChatMessage(role="system", content="You are a helpful assistant."),
            ChatMessage(role="user", content="What is RealtimeX?")
        ],
        options=ChatOptions(
            model="gpt-4o",           # Optional: specific model
            provider="openai",        # Optional: specific provider
            temperature=0.7,          # Optional: 0.0-2.0
            max_tokens=1000           # Optional: max response tokens
        )
    )
    print(response.content)

    # Streaming Chat
    async for chunk in sdk.llm.chat_stream(messages, options=options):
        print(chunk.text, end="", flush=True)
    ```
  </Tabs.Tab>
</Tabs>

#### Generate Embeddings

<Tabs items={['TypeScript', 'Python']}>
  <Tabs.Tab>
    ```typescript
    const { embeddings, dimensions, provider, model } = await sdk.llm.embed(
      ['Hello world', 'Goodbye'],
      { provider: 'openai', model: 'text-embedding-3-small' } // Optional
    );
    // embeddings: number[][] - vector arrays
    // dimensions: number - vector dimension (e.g., 1536)
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```python
    embed_result = await sdk.llm.embed(
        input=["Hello world", "Goodbye"],
        provider="openai",                    # Optional
        model="text-embedding-3-small"        # Optional
    )
    embeddings = embed_result.embeddings      # List[List[float]]
    dimensions = embed_result.dimensions      # int (e.g., 1536)
    ```
  </Tabs.Tab>
</Tabs>

#### Vector Store (RAG)

Store and search vectors for building knowledge bases:

<Tabs items={['TypeScript', 'Python']}>
  <Tabs.Tab>
    ```typescript
    // Upsert vectors with metadata
    await sdk.llm.vectors.upsert([
      { 
        id: 'chunk-1', 
        vector: embeddings[0], 
        metadata: { 
          text: 'Hello world',      // Original text (for retrieval)
          documentId: 'doc-1',       // Logical grouping
          customField: 'any value'   // Any custom metadata
        } 
      }
    ], { 
      workspaceId: 'ws-123'          // Optional: physical namespace isolation
    });

    // Query similar vectors
    const results = await sdk.llm.vectors.query(queryVector, {
      topK: 5,                       // Number of results
      workspaceId: 'ws-123',         // Optional: search in specific workspace
      filter: { documentId: 'doc-1' } // Optional: filter by document
    });
    // returns: { success, results: [{ id, score, metadata }] }

    // List all namespaces/workspaces storage for this app
    const { workspaces } = await sdk.llm.vectors.listWorkspaces();
    // returns: { success, workspaces: ['ws-123', 'default', ...] }

    // Delete all vectors in a workspace
    await sdk.llm.vectors.delete({ 
      deleteAll: true, 
      workspaceId: 'ws-123' 
    });
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```python
    # Upsert vectors with metadata
    await sdk.llm.vectors.upsert(
        vectors=[
            VectorRecord(
                id="chunk-1",
                vector=embeddings[0],
                metadata={
                    "text": "Hello world",      # Original text (for retrieval)
                    "documentId": "doc-1",       # Logical grouping
                    "customField": "any value"   # Any custom metadata
                }
            )
        ],
        workspace_id="ws-123"                   # Optional: physical namespace isolation
    )

    # Query similar vectors
    query_result = await sdk.llm.vectors.query(
        vector=embeddings[0],
        top_k=5,                                # Number of results
        workspace_id="ws-123",                  # Optional: search in specific workspace
        document_id="doc-1"                     # Optional: filter by document
    )
    # returns: VectorQueryResponse with results[]

    # List all namespaces/workspaces storage for this app
    res = await sdk.llm.vectors.list_workspaces()
    # returns: VectorListWorkspacesResponse with workspaces=['ws-123', 'default', ...]

    # Delete all vectors in a workspace
    await sdk.llm.vectors.delete(
        delete_all=True,
        workspace_id="ws-123"
    )
    ```
  </Tabs.Tab>
</Tabs>

#### High-Level Helpers (Recommended)

For common RAG patterns, use these helpers that combine embedding + storage/search:

<Tabs items={['TypeScript', 'Python']}>
  <Tabs.Tab>
    ```typescript
    // embedAndStore: Text â†’ Embed â†’ Store (one call)
    await sdk.llm.embedAndStore(
      ['Document text 1', 'Document text 2'],  // texts to embed
      {
        documentId: 'doc-123',                  // Optional: logical grouping
        workspaceId: 'ws-456',                  // Optional: physical isolation
        provider: 'openai',                     // Optional: embedding provider
        model: 'text-embedding-3-small'         // Optional: embedding model
      }
    );

    // search: Query â†’ Embed â†’ Search (one call)
    const searchResults = await sdk.llm.search(
      'What is RealtimeX?',                     // search query (text, not vector)
      {
        topK: 5,                                // Number of results
        workspaceId: 'ws-123',                  // Optional: search in workspace
        documentId: 'doc-1',                    // Optional: filter by document
        provider: 'openai',                     // Optional: embedding provider
        model: 'text-embedding-3-small'         // Optional: embedding model
      }
    );
    // returns: [{ id, score, metadata: { text, documentId, ... } }]
    
    // Use results for RAG
    const context = results.map(r => r.metadata.text).join('\n');
    const response = await sdk.llm.chat([
      { role: 'system', content: `Context:\n${context}` },
      { role: 'user', content: 'What is RealtimeX?' }
    ]);
    ```
  </Tabs.Tab>
  <Tabs.Tab>
    ```python
    # embed_and_store: Text â†’ Embed â†’ Store (one call)
    await sdk.llm.embed_and_store(
        texts=["Document text 1", "Document text 2"],  # texts to embed
        document_id="doc-123",                          # Optional: logical grouping
        workspace_id="ws-456",                          # Optional: physical isolation
        provider="openai",                              # Optional: embedding provider
        model="text-embedding-3-small"                  # Optional: embedding model
    )

    # search: Query â†’ Embed â†’ Search (one call)
    results = await sdk.llm.search(
        query="What is RealtimeX?",                     # search query (text, not vector)
        top_k=5,                                        # Number of results
        workspace_id="ws-123",                          # Optional: search in workspace
        document_id="doc-1",                            # Optional: filter by document
        provider="openai",                              # Optional: embedding provider
        model="text-embedding-3-small"                  # Optional: embedding model
    )
    # returns: List[dict] with id, score, metadata
    
    # Use results for RAG
    context = "\n".join([r["metadata"]["text"] for r in results])
    response = await sdk.llm.chat(
        messages=[
            ChatMessage(role="system", content=f"Context:\n{context}"),
            ChatMessage(role="user", content="What is RealtimeX?")
        ]
    )
    ```
  </Tabs.Tab>
</Tabs>

<Callout type="tip">
**Isolation vs Filtering:**
- **`workspaceId`**: Creates physical namespace (`sdk_{appId}_{wsId}`) - data completely isolated
- **`documentId`**: Metadata tag, filtered after search - logical grouping within workspace
</Callout>

---

## REST API Access

If you are not using one of our official SDKs, you can communicate with RealTimeX directly via our REST API.

ðŸ‘‰ [View REST API Reference](/local-apps/api-reference)
